{"version":3,"file":"static/js/vendor.hast-util-raw.2ed09d21.chunk.js","mappings":"8OAyCA,MAAMA,EACJ,+FAIIC,EAAgB,IAAIC,IAAI,CAC5B,oBACA,oBACA,oBACA,oBACA,aAIIC,EAAe,CAACC,wBAAwB,EAAMC,kBAAkB,GAa/D,SAASC,EAAIC,EAAMC,GACxB,MAAMC,EA4iBR,SAAsBC,GACpB,MAAMC,EAAqB,SAAdD,EAAKE,KAAkBF,EAAKG,SAAS,GAAKH,EACvD,OAAOI,QACLH,IACiB,YAAdA,EAAKC,MACW,YAAdD,EAAKC,MAAqD,SAA/BD,EAAKI,QAAQC,eAEjD,CAnjBmBC,CAAaV,GAExBW,GAAMC,EAAAA,EAAAA,GAAO,OAAQ,CACzBC,SAAU,CAACC,OAAMC,UAASC,OAAMC,UAASC,UAASnB,IAAKoB,GACvDC,YAIIC,EAAQ,CACZC,OAAQpB,EACJ,IAAIqB,EAAAA,GAAO3B,GACX2B,EAAAA,GAAOC,uBAAkBC,EAAW7B,GACxC8B,MAAAA,CAAOvB,GACLQ,EAAIR,EAAMkB,EACZ,EACAM,UAAU,EACV1B,QAASA,GAAW,CAAC,GAGvBU,EAAIX,EAAMqB,GACVO,EAAeP,GAAOQ,EAAAA,EAAAA,OAEtB,MAAMC,EAAK5B,EAAWmB,EAAMC,OAAOpB,SAAWmB,EAAMC,OAAOS,cACrDC,GAASC,EAAAA,EAAAA,GAAWH,EAAI,CAE5BI,KAAMb,EAAMpB,QAAQiC,OAiBtB,OAdIb,EAAMM,WACRQ,EAAAA,EAAAA,IAAMH,EAAQ,WAAW,SAAU7B,EAAMiC,EAAOC,GAC9C,MAAMC,EAAwDnC,EAC9D,GAAImC,EAAOC,MAAMD,QAAUD,QAAoBZ,IAAVW,EAAqB,CAKxD,OAHiBC,EAAO/B,SAEf8B,GAASE,EAAOC,MAAMD,OACxBF,CACT,CACF,IAKgB,SAAhBJ,EAAO3B,MACoB,IAA3B2B,EAAO1B,SAASkC,QAChBR,EAAO1B,SAAS,GAAGD,OAASL,EAAKK,KAE1B2B,EAAO1B,SAAS,GAGlB0B,CACT,CAYA,SAASS,EAAIC,EAAOrB,GAClB,IAAIe,GAAS,EAGb,GAAIM,EACF,OAASN,EAAQM,EAAMF,QACrBnB,EAAMK,OAAOgB,EAAMN,GAGzB,CAYA,SAAStB,EAAKX,EAAMkB,GAClBoB,EAAItC,EAAKG,SAAUe,EACrB,CAYA,SAASN,EAAQZ,EAAMkB,IA0UvB,SAAkBlB,EAAMkB,GACtB,MAAMb,EAAUL,EAAKK,QAAQC,cAG7B,GAAIY,EAAMC,OAAOqB,UAAUtB,QAAUuB,EAAAA,GAAcC,UAAW,OAE9DjB,EAAeP,GAAOQ,EAAAA,EAAAA,IAAW1B,IAEjC,MAAM2C,EAAUzB,EAAMC,OAAOyB,aAAaD,QAC1C,IAAIE,EAAK,iBAAkBF,EAAUA,EAAQG,aAAeC,EAAAA,EAAcC,KAEtEH,IAAOE,EAAAA,EAAcC,MAAoB,QAAZ3C,IAC/BwC,EAAKE,EAAAA,EAAcE,KAGrB,MAAMpB,GAASqB,EAAAA,EAAAA,GAEb,IAAIlD,EAAMG,SAAU,IACpB,CAACgD,MAAON,IAAOE,EAAAA,EAAcE,IAAM,MAAQ,SAIvCG,EAAM,CACVlD,KAAMmD,EAAAA,GAAAA,EAAgBC,UACtBjD,UACAkD,MAAOP,EAAAA,GAAAA,SAAc3C,GAErBmD,aAAa,EACbC,gBAAgB,EAGhBC,MAAO,UAAW7B,EAASA,EAAO6B,MAAQ,GAC1CC,SAAUC,EAAqB5D,IASjCkB,EAAMC,OAAO0C,aAAeT,EAE5BlC,EAAMC,OAAO2C,cAAc5C,EAAMC,OAAO0C,cAMxC3C,EAAMC,OAAOqB,UAAUuB,iBAAmB1D,CAG5C,CA7XE2D,CAAShE,EAAMkB,GAEfoB,EAAItC,EAAKG,SAAUe,GAuYrB,SAAgBlB,EAAMkB,GACpB,MAAMb,EAAUL,EAAKK,QAAQC,cAE7B,IACGY,EAAMC,OAAOqB,UAAUyB,eACxBC,EAAAA,EAAiBC,SAAS9D,GAE1B,OAIF,GAAIa,EAAMC,OAAOqB,UAAUtB,QAAUuB,EAAAA,GAAcC,UAAW,OAE9DjB,EAAeP,GAAOkD,EAAAA,EAAAA,GAASpE,IAG/B,MAAMoD,EAAM,CACVlD,KAAMmD,EAAAA,GAAAA,EAAgBgB,QACtBhE,UACAkD,MAAOP,EAAAA,GAAAA,SAAc3C,GACrBmD,aAAa,EACbC,gBAAgB,EAChBC,MAAO,GACPC,SAAUC,EAAqB5D,IASjCkB,EAAMC,OAAO0C,aAAeT,EAE5BlC,EAAMC,OAAO2C,cAAc5C,EAAMC,OAAO0C,cAStCxD,IAAYa,EAAMC,OAAOqB,UAAUuB,kBAElC7C,EAAMC,OAAOqB,UAAUtB,QAAUuB,EAAAA,GAAc6B,QAE9CpD,EAAMC,OAAOqB,UAAUtB,QAAUuB,EAAAA,GAAc8B,SAE/CrD,EAAMC,OAAOqB,UAAUtB,QAAUuB,EAAAA,GAAc+B,cAGjDtD,EAAMC,OAAOqB,UAAUtB,MAAQuB,EAAAA,GAAcgC,KAEjD,CA3bEC,CAAO1E,EAAMkB,EACf,CAYA,SAASL,EAAKb,EAAMkB,GAMdA,EAAMC,OAAOqB,UAAUtB,MAAQ,IACjCA,EAAMC,OAAOqB,UAAUtB,MAAQ,GAIjC,MAAMyD,EAAQ,CACZzE,KAAMmD,EAAAA,GAAAA,EAAgBuB,UACtBC,MAAO7E,EAAKoC,MACZuB,SAAUC,EAAqB5D,IAGjCyB,EAAeP,GAAOQ,EAAAA,EAAAA,IAAW1B,IAEjCkB,EAAMC,OAAO0C,aAAec,EAE5BzD,EAAMC,OAAO2C,cAAc5C,EAAMC,OAAO0C,aAC1C,CAYA,SAAS9C,EAAQf,EAAMkB,GAErB,MAAMyD,EAAQ,CACZzE,KAAMmD,EAAAA,GAAAA,EAAgByB,QACtBC,KAAM,OACNC,aAAa,EACbC,SAAU,GACVC,SAAU,GACVvB,SAAUC,EAAqB5D,IAGjCyB,EAAeP,GAAOQ,EAAAA,EAAAA,IAAW1B,IAEjCkB,EAAMC,OAAO0C,aAAec,EAE5BzD,EAAMC,OAAO2C,cAAc5C,EAAMC,OAAO0C,aAC1C,CAYA,SAAS1B,EAAOnC,EAAMkB,GAEpBA,EAAMM,UAAW,EAGjB,MAAM2D,EAyaR,SAA8BnF,GAC5B,MAAO,aAAcA,GACjBoF,EAAAA,EAAAA,IAAgB,IAAIpF,EAAMG,SAAU,MACpCiF,EAAAA,EAAAA,IAAgBpF,EACtB,CA7agBqF,CAAqBrF,GAInC,GAAI,aAAcA,GAAQ,aAAcmF,EAAO,CAE7C,MAAMG,EACJ1F,EAAI,CAACM,KAAM,OAAQC,SAAUH,EAAKG,UAAWe,EAAMpB,SAErDqF,EAAMhF,SAAWmF,EAASnF,QAC5B,CAKAW,EAAQ,CAACZ,KAAM,UAAWkC,MAAO,CAACD,OAAQgD,IAASjE,EACrD,CAYA,SAASJ,EAAQd,EAAMkB,GAGrB,MAAMqE,EAAOvF,EAAKoC,MAGZuC,EAAQ,CACZzE,KAAMmD,EAAAA,GAAAA,EAAgBmC,QACtBD,OACA5B,SAAUC,EAAqB5D,IAEjCyB,EAAeP,GAAOQ,EAAAA,EAAAA,IAAW1B,IAEjCkB,EAAMC,OAAO0C,aAAec,EAE5BzD,EAAMC,OAAO2C,cAAc5C,EAAMC,OAAO0C,aAC1C,CAYA,SAAS7C,EAAUhB,EAAMkB,GA4CvB,GAzCAA,EAAMC,OAAOqB,UAAUiD,aAAazC,KAAO,GAC3C9B,EAAMC,OAAOqB,UAAUiD,aAAaC,KAAO,EAG3CxE,EAAMC,OAAOqB,UAAUiD,aAAaE,YAAc,EAGlDzE,EAAMC,OAAOqB,UAAUiD,aAAaG,SAAW,GAG/C1E,EAAMC,OAAOqB,UAAUiD,aAAaI,iBAAkB,EACtD3E,EAAMC,OAAOqB,UAAUiD,aAAaK,kBAAmB,EACvD5E,EAAMC,OAAOqB,UAAUiD,aAAaM,eAAgB,EAGpD7E,EAAMC,OAAOqB,UAAUiD,aAAaO,OAAQ,EAG5CC,EAAS/E,GAAOQ,EAAAA,EAAAA,IAAW1B,IAE3BkB,EAAMC,OAAOqB,UAAU0D,MACrBhF,EAAMpB,QAAQqG,UACVnG,EAAKoC,MAAMgE,QAAQ9G,EAAwB,YAC3CU,EAAKoC,OACT,GAGFlB,EAAMC,OAAOqB,UAAU6D,kBAeY,KAAjCnF,EAAMC,OAAOqB,UAAUtB,OAEU,KAAjCA,EAAMC,OAAOqB,UAAUtB,MACvB,CACAA,EAAMC,OAAOqB,UAAUiD,aAAaK,kBAAmB,EAGvD,MAAMQ,EAAKpF,EAAMC,OAAOqB,UAAU+D,WAElCrF,EAAMC,OAAOqB,UAAUgE,WAAWF,EACpC,CACF,CAYA,SAASrF,EAAQwF,EAAOvF,GACtB,MAAMlB,EAA6ByG,EAEnC,IACEvF,EAAMpB,QAAQ4G,cACdxF,EAAMpB,QAAQ4G,YAAYvC,SAASnE,EAAKE,MAGnC,CACL,IAAIyG,EAAQ,GAOZ,MALIpH,EAAcqH,IAAI5G,EAAKE,QACzByG,EACE,0cAGE,IAAIE,MAAM,mBAAqB7G,EAAKE,KAAO,SAAWyG,EAC9D,CAVExE,EAAOnC,EAAMkB,EAWjB,CAYA,SAASO,EAAeP,EAAO4F,GAC7Bb,EAAS/E,EAAO4F,GAKhB,MAAMnC,EAAQzD,EAAMC,OAAOqB,UAAUuE,sBAEjCpC,GAASA,EAAMhB,WACjBgB,EAAMhB,SAASqD,QAAU9F,EAAMC,OAAOqB,UAAUiD,aAAawB,KAC7DtC,EAAMhB,SAASuD,OAAShG,EAAMC,OAAOqB,UAAUiD,aAAa0B,IAAM,EAClExC,EAAMhB,SAASyD,UAAYlG,EAAMC,OAAOqB,UAAUiD,aAAa4B,OAAS,EAExEnG,EAAMC,OAAO0C,aAAec,EAE5BzD,EAAMC,OAAO2C,cAAc5C,EAAMC,OAAO0C,eAW1C3C,EAAMC,OAAOqB,UAAU8E,QAAS,EAEhCpG,EAAMC,OAAOqB,UAAU+E,QAAS,EAIhCrG,EAAMC,OAAOqB,UAAUgF,QAAS,EAEhCtG,EAAMC,OAAOqB,UAAUiF,YAAchF,EAAAA,GAAcgC,KAEnDvD,EAAMC,OAAOqB,UAAUkF,aAAe,EAEtCxG,EAAMC,OAAOqB,UAAUmF,uBAAyB,EAEhDzG,EAAMC,OAAOqB,UAAUoF,gBAAkB,KAEzC1G,EAAMC,OAAOqB,UAAUuE,sBAAwB,KAE/C7F,EAAMC,OAAOqB,UAAUqB,aAAe,KAEtC3C,EAAMC,OAAOqB,UAAUqF,YAAc,CAAC9C,KAAM,GAAI3C,MAAO,GACzD,CAYA,SAAS6D,EAAS/E,EAAO4F,GACvB,GAAIA,QAA0BxF,IAAjBwF,EAAMO,OAAsB,CAEvC,MAAM1D,EAAW,CACfmE,UAAWhB,EAAMG,KACjBc,SAAUjB,EAAMkB,OAChBC,YAAanB,EAAMO,OACnBL,SAAU,EACVE,QAAS,EACTE,WAAY,GAKdlG,EAAMC,OAAOqB,UAAUiD,aAAayC,aAA+B,EAAfpB,EAAMkB,OAC1D9G,EAAMC,OAAOqB,UAAUiD,aAAa0C,kBAAoBrB,EAAMO,OAC9DnG,EAAMC,OAAOqB,UAAUiD,aAAawB,KAAOH,EAAMG,KAEjD/F,EAAMC,OAAOqB,UAAUoF,gBAAkBjE,CAC3C,CACF,CA6JA,SAASC,EAAqB5D,GAC5B,MAAMoI,GAAQ1G,EAAAA,EAAAA,IAAW1B,IAAS,CAChCiH,UAAM3F,EACN0G,YAAQ1G,EACR+F,YAAQ/F,GAEJ+G,GAAMjE,EAAAA,EAAAA,GAASpE,IAAS,CAC5BiH,UAAM3F,EACN0G,YAAQ1G,EACR+F,YAAQ/F,GAeV,MAXiB,CACfwG,UAAWM,EAAMnB,KACjBc,SAAUK,EAAMJ,OAChBC,YAAaG,EAAMf,OACnBL,QAASqB,EAAIpB,KACbC,OAAQmB,EAAIL,OACZZ,UAAWiB,EAAIhB,OAMnB,C","sources":["../node_modules/hast-util-raw/lib/index.js"],"sourcesContent":["/**\n * @import {Options} from 'hast-util-raw'\n * @import {Comment, Doctype, Element, Nodes, RootContent, Root, Text} from 'hast'\n * @import {Raw} from 'mdast-util-to-hast'\n * @import {DefaultTreeAdapterMap, ParserOptions} from 'parse5'\n * @import {Point} from 'unist'\n */\n\n/**\n * @typedef State\n *   Info passed around about the current state.\n * @property {(node: Nodes) => undefined} handle\n *   Add a hast node to the parser.\n * @property {Options} options\n *   User configuration.\n * @property {Parser<DefaultTreeAdapterMap>} parser\n *   Current parser.\n * @property {boolean} stitches\n *   Whether there are stitches.\n */\n\n/**\n * @typedef Stitch\n *   Custom comment-like value we pass through parse5, which contains a\n *   replacement node that weâ€™ll swap back in afterwards.\n * @property {'comment'} type\n *   Node type.\n * @property {{stitch: Nodes}} value\n *   Replacement value.\n */\n\nimport structuredClone from '@ungap/structured-clone'\nimport {fromParse5} from 'hast-util-from-parse5'\nimport {toParse5} from 'hast-util-to-parse5'\nimport {htmlVoidElements} from 'html-void-elements'\nimport {Parser, Token, TokenizerMode, html} from 'parse5'\nimport {pointEnd, pointStart} from 'unist-util-position'\nimport {visit} from 'unist-util-visit'\nimport {webNamespaces} from 'web-namespaces'\nimport {zwitch} from 'zwitch'\n\nconst gfmTagfilterExpression =\n  /<(\\/?)(iframe|noembed|noframes|plaintext|script|style|textarea|title|xmp)(?=[\\t\\n\\f\\r />])/gi\n\n// Node types associated with MDX.\n// <https://github.com/mdx-js/mdx/blob/8a56312/packages/mdx/lib/node-types.js>\nconst knownMdxNames = new Set([\n  'mdxFlowExpression',\n  'mdxJsxFlowElement',\n  'mdxJsxTextElement',\n  'mdxTextExpression',\n  'mdxjsEsm'\n])\n\n/** @type {ParserOptions<DefaultTreeAdapterMap>} */\nconst parseOptions = {sourceCodeLocationInfo: true, scriptingEnabled: false}\n\n/**\n * Pass a hast tree through an HTML parser, which will fix nesting, and turn\n * raw nodes into actual nodes.\n *\n * @param {Nodes} tree\n *   Original hast tree to transform.\n * @param {Options | null | undefined} [options]\n *   Configuration (optional).\n * @returns {Nodes}\n *   Parsed again tree.\n */\nexport function raw(tree, options) {\n  const document = documentMode(tree)\n  /** @type {(node: Nodes, state: State) => undefined} */\n  const one = zwitch('type', {\n    handlers: {root, element, text, comment, doctype, raw: handleRaw},\n    unknown\n  })\n\n  /** @type {State} */\n  const state = {\n    parser: document\n      ? new Parser(parseOptions)\n      : Parser.getFragmentParser(undefined, parseOptions),\n    handle(node) {\n      one(node, state)\n    },\n    stitches: false,\n    options: options || {}\n  }\n\n  one(tree, state)\n  resetTokenizer(state, pointStart())\n\n  const p5 = document ? state.parser.document : state.parser.getFragment()\n  const result = fromParse5(p5, {\n    // To do: support `space`?\n    file: state.options.file\n  })\n\n  if (state.stitches) {\n    visit(result, 'comment', function (node, index, parent) {\n      const stitch = /** @type {Stitch} */ (/** @type {unknown} */ (node))\n      if (stitch.value.stitch && parent && index !== undefined) {\n        /** @type {Array<RootContent>} */\n        const siblings = parent.children\n        // @ts-expect-error: assume the stitch is allowed.\n        siblings[index] = stitch.value.stitch\n        return index\n      }\n    })\n  }\n\n  // Unpack if possible and when not given a `root`.\n  if (\n    result.type === 'root' &&\n    result.children.length === 1 &&\n    result.children[0].type === tree.type\n  ) {\n    return result.children[0]\n  }\n\n  return result\n}\n\n/**\n * Transform all nodes\n *\n * @param {Array<RootContent>} nodes\n *   hast content.\n * @param {State} state\n *   Info passed around about the current state.\n * @returns {undefined}\n *   Nothing.\n */\nfunction all(nodes, state) {\n  let index = -1\n\n  /* istanbul ignore else - invalid nodes, see rehypejs/rehype-raw#7. */\n  if (nodes) {\n    while (++index < nodes.length) {\n      state.handle(nodes[index])\n    }\n  }\n}\n\n/**\n * Transform a root.\n *\n * @param {Root} node\n *   hast root node.\n * @param {State} state\n *   Info passed around about the current state.\n * @returns {undefined}\n *   Nothing.\n */\nfunction root(node, state) {\n  all(node.children, state)\n}\n\n/**\n * Transform an element.\n *\n * @param {Element} node\n *   hast element node.\n * @param {State} state\n *   Info passed around about the current state.\n * @returns {undefined}\n *   Nothing.\n */\nfunction element(node, state) {\n  startTag(node, state)\n\n  all(node.children, state)\n\n  endTag(node, state)\n}\n\n/**\n * Transform a text.\n *\n * @param {Text} node\n *   hast text node.\n * @param {State} state\n *   Info passed around about the current state.\n * @returns {undefined}\n *   Nothing.\n */\nfunction text(node, state) {\n  // Allow `DATA` through `PLAINTEXT`,\n  // but when hanging in a tag for example,\n  // switch back to `DATA`.\n  // Note: `State` is not exposed by `parse5`, so these numbers are fragile.\n  // See: <https://github.com/inikulin/parse5/blob/46cba43/packages/parse5/lib/tokenizer/index.ts#L58>\n  if (state.parser.tokenizer.state > 4) {\n    state.parser.tokenizer.state = 0\n  }\n\n  /** @type {Token.CharacterToken} */\n  const token = {\n    type: Token.TokenType.CHARACTER,\n    chars: node.value,\n    location: createParse5Location(node)\n  }\n\n  resetTokenizer(state, pointStart(node))\n  // @ts-expect-error: private.\n  state.parser.currentToken = token\n  // @ts-expect-error: private.\n  state.parser._processToken(state.parser.currentToken)\n}\n\n/**\n * Transform a doctype.\n *\n * @param {Doctype} node\n *   hast doctype node.\n * @param {State} state\n *   Info passed around about the current state.\n * @returns {undefined}\n *   Nothing.\n */\nfunction doctype(node, state) {\n  /** @type {Token.DoctypeToken} */\n  const token = {\n    type: Token.TokenType.DOCTYPE,\n    name: 'html',\n    forceQuirks: false,\n    publicId: '',\n    systemId: '',\n    location: createParse5Location(node)\n  }\n\n  resetTokenizer(state, pointStart(node))\n  // @ts-expect-error: private.\n  state.parser.currentToken = token\n  // @ts-expect-error: private.\n  state.parser._processToken(state.parser.currentToken)\n}\n\n/**\n * Transform a stitch.\n *\n * @param {Nodes} node\n *   unknown node.\n * @param {State} state\n *   Info passed around about the current state.\n * @returns {undefined}\n *   Nothing.\n */\nfunction stitch(node, state) {\n  // Mark that there are stitches, so we need to walk the tree and revert them.\n  state.stitches = true\n\n  /** @type {Nodes} */\n  const clone = cloneWithoutChildren(node)\n\n  // Recurse, because to somewhat handle `[<x>]</x>` (where `[]` denotes the\n  // passed through node).\n  if ('children' in node && 'children' in clone) {\n    // Root in root out.\n    const fakeRoot = /** @type {Root} */ (\n      raw({type: 'root', children: node.children}, state.options)\n    )\n    clone.children = fakeRoot.children\n  }\n\n  // Hack: `value` is supposed to be a string, but as none of the tools\n  // (`parse5` or `hast-util-from-parse5`) looks at it, we can pass nodes\n  // through.\n  comment({type: 'comment', value: {stitch: clone}}, state)\n}\n\n/**\n * Transform a comment (or stitch).\n *\n * @param {Comment | Stitch} node\n *   hast comment node.\n * @param {State} state\n *   Info passed around about the current state.\n * @returns {undefined}\n *   Nothing.\n */\nfunction comment(node, state) {\n  /** @type {string} */\n  // @ts-expect-error: we pass stitches through.\n  const data = node.value\n\n  /** @type {Token.CommentToken} */\n  const token = {\n    type: Token.TokenType.COMMENT,\n    data,\n    location: createParse5Location(node)\n  }\n  resetTokenizer(state, pointStart(node))\n  // @ts-expect-error: private.\n  state.parser.currentToken = token\n  // @ts-expect-error: private.\n  state.parser._processToken(state.parser.currentToken)\n}\n\n/**\n * Transform a raw node.\n *\n * @param {Raw} node\n *   hast raw node.\n * @param {State} state\n *   Info passed around about the current state.\n * @returns {undefined}\n *   Nothing.\n */\nfunction handleRaw(node, state) {\n  // Reset preprocessor:\n  // See: <https://github.com/inikulin/parse5/blob/6f7ca60/packages/parse5/lib/tokenizer/preprocessor.ts#L18-L31>.\n  state.parser.tokenizer.preprocessor.html = ''\n  state.parser.tokenizer.preprocessor.pos = -1\n  // @ts-expect-error: private.\n  // type-coverage:ignore-next-line\n  state.parser.tokenizer.preprocessor.lastGapPos = -2\n  // @ts-expect-error: private.\n  // type-coverage:ignore-next-line\n  state.parser.tokenizer.preprocessor.gapStack = []\n  // @ts-expect-error: private.\n  // type-coverage:ignore-next-line\n  state.parser.tokenizer.preprocessor.skipNextNewLine = false\n  state.parser.tokenizer.preprocessor.lastChunkWritten = false\n  state.parser.tokenizer.preprocessor.endOfChunkHit = false\n  // @ts-expect-error: private.\n  // type-coverage:ignore-next-line\n  state.parser.tokenizer.preprocessor.isEol = false\n\n  // Now pass `node.value`.\n  setPoint(state, pointStart(node))\n\n  state.parser.tokenizer.write(\n    state.options.tagfilter\n      ? node.value.replace(gfmTagfilterExpression, '&lt;$1$2')\n      : node.value,\n    false\n  )\n  // @ts-expect-error: private.\n  state.parser.tokenizer._runParsingLoop()\n\n  // Character references hang, so if we ended there, we need to flush\n  // those too.\n  // We reset the preprocessor as if the document ends here.\n  // Then one single call to the relevant state does the trick, parse5\n  // consumes the whole token.\n\n  // Note: `State` is not exposed by `parse5`, so these numbers are fragile.\n  // See: <https://github.com/inikulin/parse5/blob/46cba43/packages/parse5/lib/tokenizer/index.ts#L58>\n  // Note: a change to `parse5`, which breaks this, was merged but not released.\n  // Investigate when it is.\n  // To do: remove next major.\n  /* c8 ignore next 12 -- removed in <https://github.com/inikulin/parse5/pull/897> */\n  if (\n    state.parser.tokenizer.state === 72 /* NAMED_CHARACTER_REFERENCE */ ||\n    // @ts-expect-error: removed.\n    state.parser.tokenizer.state === 78 /* NUMERIC_CHARACTER_REFERENCE_END */\n  ) {\n    state.parser.tokenizer.preprocessor.lastChunkWritten = true\n    /** @type {number} */\n    // @ts-expect-error: private.\n    const cp = state.parser.tokenizer._consume()\n    // @ts-expect-error: private.\n    state.parser.tokenizer._callState(cp)\n  }\n}\n\n/**\n * Crash on an unknown node.\n *\n * @param {unknown} node_\n *   unknown node.\n * @param {State} state\n *   Info passed around about the current state.\n * @returns {undefined}\n *   Never.\n */\nfunction unknown(node_, state) {\n  const node = /** @type {Nodes} */ (node_)\n\n  if (\n    state.options.passThrough &&\n    state.options.passThrough.includes(node.type)\n  ) {\n    stitch(node, state)\n  } else {\n    let extra = ''\n\n    if (knownMdxNames.has(node.type)) {\n      extra =\n        \". It looks like you are using MDX nodes with `hast-util-raw` (or `rehype-raw`). If you use this because you are using remark or rehype plugins that inject `'html'` nodes, then please raise an issue with that plugin, as its a bad and slow idea. If you use this because you are using markdown syntax, then you have to configure this utility (or plugin) to pass through these nodes (see `passThrough` in docs), but you can also migrate to use the MDX syntax\"\n    }\n\n    throw new Error('Cannot compile `' + node.type + '` node' + extra)\n  }\n}\n\n/**\n * Reset the tokenizer of a parser.\n *\n * @param {State} state\n *   Info passed around about the current state.\n * @param {Point | undefined} point\n *   Point.\n * @returns {undefined}\n *   Nothing.\n */\nfunction resetTokenizer(state, point) {\n  setPoint(state, point)\n\n  // Process final characters if theyâ€™re still there after hibernating.\n  /** @type {Token.CharacterToken} */\n  // @ts-expect-error: private.\n  const token = state.parser.tokenizer.currentCharacterToken\n\n  if (token && token.location) {\n    token.location.endLine = state.parser.tokenizer.preprocessor.line\n    token.location.endCol = state.parser.tokenizer.preprocessor.col + 1\n    token.location.endOffset = state.parser.tokenizer.preprocessor.offset + 1\n    // @ts-expect-error: private.\n    state.parser.currentToken = token\n    // @ts-expect-error: private.\n    state.parser._processToken(state.parser.currentToken)\n  }\n\n  // Reset tokenizer:\n  // See: <https://github.com/inikulin/parse5/blob/6f7ca60/packages/parse5/lib/tokenizer/index.ts#L187-L223>.\n  // Especially putting it back in the `data` state is useful: some elements,\n  // like textareas and iframes, change the state.\n  // See GH-7.\n  // But also if broken HTML is in `raw`, and then a correct element is given.\n  // See GH-11.\n  // @ts-expect-error: private.\n  state.parser.tokenizer.paused = false\n  // @ts-expect-error: private.\n  state.parser.tokenizer.inLoop = false\n\n  // Note: donâ€™t reset `state`, `inForeignNode`, or `lastStartTagName`, we\n  // manually update those when needed.\n  state.parser.tokenizer.active = false\n  // @ts-expect-error: private.\n  state.parser.tokenizer.returnState = TokenizerMode.DATA\n  // @ts-expect-error: private.\n  state.parser.tokenizer.charRefCode = -1\n  // @ts-expect-error: private.\n  state.parser.tokenizer.consumedAfterSnapshot = -1\n  // @ts-expect-error: private.\n  state.parser.tokenizer.currentLocation = null\n  // @ts-expect-error: private.\n  state.parser.tokenizer.currentCharacterToken = null\n  // @ts-expect-error: private.\n  state.parser.tokenizer.currentToken = null\n  // @ts-expect-error: private.\n  state.parser.tokenizer.currentAttr = {name: '', value: ''}\n}\n\n/**\n * Set current location.\n *\n * @param {State} state\n *   Info passed around about the current state.\n * @param {Point | undefined} point\n *   Point.\n * @returns {undefined}\n *   Nothing.\n */\nfunction setPoint(state, point) {\n  if (point && point.offset !== undefined) {\n    /** @type {Token.Location} */\n    const location = {\n      startLine: point.line,\n      startCol: point.column,\n      startOffset: point.offset,\n      endLine: -1,\n      endCol: -1,\n      endOffset: -1\n    }\n\n    // @ts-expect-error: private.\n    // type-coverage:ignore-next-line\n    state.parser.tokenizer.preprocessor.lineStartPos = -point.column + 1 // Looks weird, but ensures we get correct positional info.\n    state.parser.tokenizer.preprocessor.droppedBufferSize = point.offset\n    state.parser.tokenizer.preprocessor.line = point.line\n    // @ts-expect-error: private.\n    state.parser.tokenizer.currentLocation = location\n  }\n}\n\n/**\n * Emit a start tag.\n *\n * @param {Element} node\n *   Element.\n * @param {State} state\n *   Info passed around about the current state.\n * @returns {undefined}\n *   Nothing.\n */\nfunction startTag(node, state) {\n  const tagName = node.tagName.toLowerCase()\n\n  // Ignore tags if weâ€™re in plain text.\n  if (state.parser.tokenizer.state === TokenizerMode.PLAINTEXT) return\n\n  resetTokenizer(state, pointStart(node))\n\n  const current = state.parser.openElements.current\n  let ns = 'namespaceURI' in current ? current.namespaceURI : webNamespaces.html\n\n  if (ns === webNamespaces.html && tagName === 'svg') {\n    ns = webNamespaces.svg\n  }\n\n  const result = toParse5(\n    // Shallow clone to not delve into `children`: we only need the attributes.\n    {...node, children: []},\n    {space: ns === webNamespaces.svg ? 'svg' : 'html'}\n  )\n\n  /** @type {Token.TagToken} */\n  const tag = {\n    type: Token.TokenType.START_TAG,\n    tagName,\n    tagID: html.getTagID(tagName),\n    // We always send start and end tags.\n    selfClosing: false,\n    ackSelfClosing: false,\n    // Always element.\n    /* c8 ignore next */\n    attrs: 'attrs' in result ? result.attrs : [],\n    location: createParse5Location(node)\n  }\n\n  // The HTML parsing algorithm works by doing half of the state management in\n  // the tokenizer and half in the parser.\n  // We canâ€™t use the tokenizer here, as we donâ€™t have strings.\n  // So we act *as if* the tokenizer emits tokens:\n\n  // @ts-expect-error: private.\n  state.parser.currentToken = tag\n  // @ts-expect-error: private.\n  state.parser._processToken(state.parser.currentToken)\n\n  // â€¦but then we still need a bunch of work that the tokenizer would normally\n  // do, such as:\n\n  // Set a tag name, similar to how the tokenizer would do it.\n  state.parser.tokenizer.lastStartTagName = tagName\n\n  // `inForeignNode` is correctly set by the parser.\n}\n\n/**\n * Emit an end tag.\n *\n * @param {Element} node\n *   Element.\n * @param {State} state\n *   Info passed around about the current state.\n * @returns {undefined}\n *   Nothing.\n */\nfunction endTag(node, state) {\n  const tagName = node.tagName.toLowerCase()\n  // Do not emit closing tags for HTML void elements.\n  if (\n    !state.parser.tokenizer.inForeignNode &&\n    htmlVoidElements.includes(tagName)\n  ) {\n    return\n  }\n\n  // Ignore tags if weâ€™re in plain text.\n  if (state.parser.tokenizer.state === TokenizerMode.PLAINTEXT) return\n\n  resetTokenizer(state, pointEnd(node))\n\n  /** @type {Token.TagToken} */\n  const tag = {\n    type: Token.TokenType.END_TAG,\n    tagName,\n    tagID: html.getTagID(tagName),\n    selfClosing: false,\n    ackSelfClosing: false,\n    attrs: [],\n    location: createParse5Location(node)\n  }\n\n  // The HTML parsing algorithm works by doing half of the state management in\n  // the tokenizer and half in the parser.\n  // We canâ€™t use the tokenizer here, as we donâ€™t have strings.\n  // So we act *as if* the tokenizer emits tokens:\n\n  // @ts-expect-error: private.\n  state.parser.currentToken = tag\n  // @ts-expect-error: private.\n  state.parser._processToken(state.parser.currentToken)\n\n  // â€¦but then we still need a bunch of work that the tokenizer would normally\n  // do, such as:\n\n  // Switch back to the data state after alternative states that donâ€™t accept\n  // tags:\n  if (\n    // Current element is closed.\n    tagName === state.parser.tokenizer.lastStartTagName &&\n    // `<textarea>` and `<title>`\n    (state.parser.tokenizer.state === TokenizerMode.RCDATA ||\n      // `<iframe>`, `<noembed>`, `<noframes>`, `<style>`, `<xmp>`\n      state.parser.tokenizer.state === TokenizerMode.RAWTEXT ||\n      // `<script>`\n      state.parser.tokenizer.state === TokenizerMode.SCRIPT_DATA)\n    // Note: `<plaintext>` not needed, as itâ€™s the last element.\n  ) {\n    state.parser.tokenizer.state = TokenizerMode.DATA\n  }\n}\n\n/**\n * Check if `node` represents a whole document or a fragment.\n *\n * @param {Nodes} node\n *   hast node.\n * @returns {boolean}\n *   Whether this represents a whole document or a fragment.\n */\nfunction documentMode(node) {\n  const head = node.type === 'root' ? node.children[0] : node\n  return Boolean(\n    head &&\n      (head.type === 'doctype' ||\n        (head.type === 'element' && head.tagName.toLowerCase() === 'html'))\n  )\n}\n\n/**\n * Get a `parse5` location from a node.\n *\n * @param {Nodes | Stitch} node\n *   hast node.\n * @returns {Token.Location}\n *   `parse5` location.\n */\nfunction createParse5Location(node) {\n  const start = pointStart(node) || {\n    line: undefined,\n    column: undefined,\n    offset: undefined\n  }\n  const end = pointEnd(node) || {\n    line: undefined,\n    column: undefined,\n    offset: undefined\n  }\n\n  /** @type {Record<keyof Token.Location, number | undefined>} */\n  const location = {\n    startLine: start.line,\n    startCol: start.column,\n    startOffset: start.offset,\n    endLine: end.line,\n    endCol: end.column,\n    endOffset: end.offset\n  }\n\n  // @ts-expect-error: unist point values can be `undefined` in hast, which\n  // `parse5` types donâ€™t want.\n  return location\n}\n\n/**\n * @template {Nodes} NodeType\n *   Node type.\n * @param {NodeType} node\n *   Node to clone.\n * @returns {NodeType}\n *   Cloned node, without children.\n */\nfunction cloneWithoutChildren(node) {\n  return 'children' in node\n    ? structuredClone({...node, children: []})\n    : structuredClone(node)\n}\n"],"names":["gfmTagfilterExpression","knownMdxNames","Set","parseOptions","sourceCodeLocationInfo","scriptingEnabled","raw","tree","options","document","node","head","type","children","Boolean","tagName","toLowerCase","documentMode","one","zwitch","handlers","root","element","text","comment","doctype","handleRaw","unknown","state","parser","Parser","getFragmentParser","undefined","handle","stitches","resetTokenizer","pointStart","p5","getFragment","result","fromParse5","file","visit","index","parent","stitch","value","length","all","nodes","tokenizer","TokenizerMode","PLAINTEXT","current","openElements","ns","namespaceURI","webNamespaces","html","svg","toParse5","space","tag","Token","START_TAG","tagID","selfClosing","ackSelfClosing","attrs","location","createParse5Location","currentToken","_processToken","lastStartTagName","startTag","inForeignNode","htmlVoidElements","includes","pointEnd","END_TAG","RCDATA","RAWTEXT","SCRIPT_DATA","DATA","endTag","token","CHARACTER","chars","DOCTYPE","name","forceQuirks","publicId","systemId","clone","structuredClone","cloneWithoutChildren","fakeRoot","data","COMMENT","preprocessor","pos","lastGapPos","gapStack","skipNextNewLine","lastChunkWritten","endOfChunkHit","isEol","setPoint","write","tagfilter","replace","_runParsingLoop","cp","_consume","_callState","node_","passThrough","extra","has","Error","point","currentCharacterToken","endLine","line","endCol","col","endOffset","offset","paused","inLoop","active","returnState","charRefCode","consumedAfterSnapshot","currentLocation","currentAttr","startLine","startCol","column","startOffset","lineStartPos","droppedBufferSize","start","end"],"sourceRoot":""}