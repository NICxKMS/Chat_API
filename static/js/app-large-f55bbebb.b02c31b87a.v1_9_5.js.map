{"version":3,"file":"static/js/app-large-f55bbebb.b02c31b87a.v1_9_5.js","mappings":"0TAcA,MAAMA,GAAyBC,EAAAA,EAAAA,MAElBC,EAAqBA,KAChC,MAAMC,GAAUC,EAAAA,EAAAA,IAAWJ,GAC3B,QAAgBK,IAAZF,EACF,MAAM,IAAIG,MAAM,oEAElB,OAAOH,CAAO,EAGHI,EAA0BC,IAAmB,IAAlB,SAAEC,GAAUD,EAClD,MAAM,OAAEE,IAAWC,EAAAA,EAAAA,MACb,cAAEC,IAAkBC,EAAAA,EAAAA,OACpB,yBAAEC,IAA6BC,EAAAA,EAAAA,MAC/B,QAAEC,IAAYC,EAAAA,EAAAA,MACd,eAAEC,EAAc,eAAEC,EAAc,oBAAEC,EAAmB,sBAAEC,IAA0BC,EAAAA,EAAAA,MACjF,wBAAEC,EAAuB,SAAEC,IAAaC,EAAAA,EAAAA,MACxC,wBAAEC,EAAuB,sBAAEC,EAAqB,yBAAEC,IAA6BC,EAAAA,EAAAA,KAG/EC,GAAmBC,EAAAA,EAAAA,IAAO,IAC1BC,GAAsBD,EAAAA,EAAAA,IAAO,MAC7BE,GAAqBF,EAAAA,EAAAA,IAAO,MAC5BG,GAAiBH,EAAAA,EAAAA,KAAO,GACxBI,GAAwBJ,EAAAA,EAAAA,KAAO,GAG/BK,GAAsBC,EAAAA,EAAAA,KAC1B,IAAMC,KAAUC,GAAYlB,EAAsBkB,IAAU,KAC5D,CAAClB,IAKGmB,IADqBT,EAAAA,EAAAA,IAAO,OACVA,EAAAA,EAAAA,IAAO,OACzBU,GAA0BC,EAAAA,EAAAA,KAAY,KACrCF,EAAgBG,UACnBH,EAAgBG,QAAU,IAAIC,MAEzBJ,EAAgBG,UACtB,IAEGE,GAAmBH,EAAAA,EAAAA,KAAaI,GAAU,IAAIC,SAAQ,CAACC,EAASC,KACpE,MAAMC,EAAST,IACfS,EAAOC,UAAaC,GAAMJ,EAAQI,EAAEC,MACpCH,EAAOI,QAAUL,EACjBC,EAAOK,YAAYT,EAAM,KACvB,CAACL,IAGCe,GAAyBd,EAAAA,EAAAA,KAAYe,eAAOC,GAA+B,IAAtBC,EAASC,UAAAC,OAAA,QAAAxD,IAAAuD,UAAA,GAAAA,UAAA,GAAG,KAErE,MAAME,EAA+B,oBAAXC,QAA0BA,OAAOC,WACvDD,OAAOC,aACPC,KAAKC,SAASC,SAAS,IAAIC,UAAU,GAAKC,KAAKC,MAAMH,SAAS,IAClEnC,EAAoBW,QAAUmB,EAC9B,MAAMS,EAA0B,OAAdZ,GAAsBa,OAAOC,UAAUd,IAAcA,GAAa,EACpF,IAAKD,IAAY9C,EAEf,OADAY,EAAS,6CACF,KAET,MAAMkD,EAAU,GAAG9D,EAAc+D,YAAY/D,EAAcgE,KAC3D,IAAKF,EAEH,OADAlD,EAAS,2BACF,KAET,IAAIqD,EACAN,EACFpD,GAAe2D,IACb,MAAMC,EAAYD,EAAKE,MAAM,EAAGrB,GAC1BsB,EAAWH,EAAKnB,GAGtB,OADAkB,EAAc,IAAKI,EAAU1C,QAASmB,GAC/B,IAAIqB,EAAWF,EAAY,IAGpCA,EAAczD,EAAoB,OAAQsC,GAE5ChC,IACAC,IACAQ,EAAsBQ,SAAU,EAChCpB,GAAwB,GACxBC,EAAS,MACTM,EAAiBa,QAAU,GAC3BT,EAAeS,SAAU,EACzBvB,EAAoB,YAAa,IACjC,IAAI8D,EAAYC,YAAW,KAAO,IAADC,EACL,QAA1BA,EAAAnD,EAAmBU,eAAO,IAAAyC,GAA1BA,EAA4BC,MAAM,WAClC7D,EAAS,wBACTD,GAAwB,EAAM,GAC7B,KACH,MAAM+D,EAAkB,IAAIC,gBAC5BtD,EAAmBU,QAAU2C,EAC7B,IACE,MAAME,EAAW1E,EAAyBF,GAEpC6E,EAAgBvE,EAAeyB,QAClCqC,MAAM,GAAI,GACVU,KAAIC,IAAA,IAAC,QAAEC,KAAYC,GAAGF,EAAA,OAAKE,CAAC,KAC3BL,EAASM,cAAkBL,EAAc5B,QAAoC,WAA1B4B,EAAc,GAAGM,MACtEN,EAAcO,QAAQ,CAAED,KAAM,SAAUxD,QAASiD,EAASM,aAAcG,UAAW5B,KAAKC,MAAQ,IAElG,MAAM4B,EAAU,CACdpC,YACAqC,MAAOzB,EACP0B,SAAUX,EACVY,YAAab,EAASa,YACtBC,WAAYd,EAASc,WACrBC,MAAOf,EAASe,MAChBC,kBAAmBhB,EAASgB,kBAC5BC,iBAAkBjB,EAASiB,kBAEvBC,EAAU,CAAE,eAAgB,mBAAoB,OAAU,oBAAqB,gBAAiB,YAClG1F,IAAS0F,EAAuB,cAAI,UAAU1F,KAClD,MAAM2F,QAAiBC,EAAAA,EAAAA,GAAe,IAAIC,IAAI,mBAAoBnG,GAAQyD,WAAY,CACpF2C,OAAQ,OAAQJ,UAASK,KAAMC,KAAKC,UAAUf,GAAUgB,OAAQ5B,EAAgB4B,OAAQC,MAAO,aAEjG,IAAKR,EAASS,GAAI,MAAM,IAAI9G,MAAM,cAAcqG,EAASU,UACzD,MAAMC,EAASX,EAASI,KAAKQ,YACvBC,EAAU,IAAIC,YAAY,SAChC,IAAIC,EAAqB,GACzB,OAAa,CACX,MAAM,KAAEC,EAAI,MAAEC,SAAgBN,EAAOO,OAOrC,GANAC,aAAa5C,GACbA,EAAYC,YAAW,KAAO,IAAD4C,EACD,QAA1BA,EAAA9F,EAAmBU,eAAO,IAAAoF,GAA1BA,EAA4B1C,QAC5B7D,EAAS,wBACTD,GAAwB,EAAM,GAC7B,KACCoG,EAEF,MAEF,MAAM7E,EAAQ0E,EAAQQ,OAAOJ,EAAO,CAAEK,QAAQ,IAI9C,IACE,MAAMC,QAAarF,EAAiBC,GACpC,IAAK,MAAMqF,KAAOD,EAAM,CAAC,IAADE,EAAAC,EAAAC,EAEtB,GAAgB,QAAZF,EAAAD,EAAII,gBAAQ,IAAAH,GAAZA,EAAcI,OAA8B,UAArBL,EAAIM,aAA0B,CAAC,IAADC,EAAAC,EACvD,MAAMC,GAAqB,QAAZF,EAAAP,EAAII,gBAAQ,IAAAG,GAAO,QAAPC,EAAZD,EAAcF,aAAK,IAAAG,OAAP,EAAZA,EAAqBjF,UAAW,mCAe/C,OAbAlC,EAASoH,GACTzH,GAAe2D,IACb,MAAM+D,EAAa,IAAI/D,GACjBgE,EAAUD,EAAWA,EAAWhF,OAAS,GAQ/C,OAPIiF,GAA4B,cAAjBA,EAAQ/C,OACrB+C,EAAQvG,SAAW,kBAAkBqG,IACjCE,EAAQlD,UACVkD,EAAQlD,QAAQmD,YAAa,EAC7BD,EAAQlD,QAAQ4C,OAAQ,IAGrBK,CAAU,IAEZ,IACT,CAEIV,EAAI5F,UAEDJ,EAAsBQ,UACzBf,EAAyB,GACzBO,EAAsBQ,SAAU,GAElC+E,GAAsBS,EAAI5F,QAC1BT,EAAiBa,QAAU+E,EAC3BtF,EAAoBsF,IAGtB,MAAMsB,EAA8C,QAA9BX,EAAY,QAAZC,EAAGH,EAAIc,aAAK,IAAAX,OAAA,EAATA,EAAWU,wBAAgB,IAAAX,EAAAA,EAAI,EACxDzG,EAAyBoH,EAAkBb,EAAIe,OAAQf,EAAIc,MAAOd,EAAIM,aACxE,CACF,CAAE,MAAO,CACX,CAGA,OAFArG,EAAoB+G,QACpB9H,EAAsBS,EAAiBa,SAChCb,EAAiBa,OAC1B,CAAE,MAAO6F,GAgBP,OAdAhH,EAASgH,EAAM9E,SAEfvC,GAAe2D,IACb,MAAM+D,EAAa,IAAI/D,GACjBgE,EAAUD,EAAWA,EAAWhF,OAAS,GAQ/C,OAPIiF,GAA4B,cAAjBA,EAAQ/C,OACrB+C,EAAQvG,SAAW,kBAAkBiG,EAAM9E,SAAW,qCAClDoF,EAAQlD,UACVkD,EAAQlD,QAAQmD,YAAa,EAC7BD,EAAQlD,QAAQ4C,OAAQ,IAGrBK,CAAU,IAEZ,IACT,CAAC,QACCf,aAAa5C,GACbhD,EAAeS,SAAU,EACzBpB,GAAwB,GAExBS,EAAoBW,QAAU,IAChC,CACF,GAAG,CACDjC,EAAQE,EAAeE,EAA0BE,EACjDE,EAAgBC,EAAgBC,EAAqBC,EACrDe,EAAqBZ,EAAUD,EAC/BG,EAAyBC,EAAuBC,EAChDiB,IAGIuG,GAAgB1G,EAAAA,EAAAA,KAAYe,UAC5BxB,EAAmBU,SAASV,EAAmBU,QAAQ0C,MAAM,gBACjE,MAAMgE,EAAQrH,EAAoBW,QAClC,GAAI0G,EAAO,CACT,MAAM3C,EAAU,CAAE,eAAgB,oBAC9B1F,IAAS0F,EAAuB,cAAI,UAAU1F,KAClD,UACQ4F,EAAAA,EAAAA,GAAe,IAAIC,IAAI,iBAAkBnG,GAAQyD,WAAY,CACjE2C,OAAQ,OAAQJ,UAASK,KAAMC,KAAKC,UAAU,CAAEnD,UAAWuF,KAE/D,CAAE,MAAO,CAAC,QACRrH,EAAoBW,QAAU,KAC9BV,EAAmBU,QAAU,IAC/B,CACF,CAGA,OAFAT,EAAeS,SAAU,EACzBpB,GAAwB,IACjB,CAAI,GACV,CAACb,EAAQM,EAASO,IAEfqG,GAAQvF,EAAAA,EAAAA,KAAQ,MACpBmB,yBACA4F,gBACAvG,mBACAf,mBACAwH,YAAaA,IAAMpH,EAAeS,WAChC,CAACa,EAAwB4F,EAAevG,IAE5C,OACE0G,EAAAA,EAAAA,GAACvJ,EAAuBwJ,SAAQ,CAAC5B,MAAOA,EAAMnH,SAC3CA,GAC+B,C,+EC5PtC,MAAMgJ,EAAmB,CACvBpD,YAAa,GACbE,MAAO,EACPD,WAAY,KACZE,kBAAmB,EACnBC,iBAAkB,EAClBiD,WAAW,EACX5D,aAAc,khKAmHV6D,GAAkB1J,EAAAA,EAAAA,MAGXc,EAAcA,KACzB,MAAMZ,GAAUC,EAAAA,EAAAA,IAAWuJ,GAC3B,QAAgBtJ,IAAZF,EACF,MAAM,IAAIG,MAAM,sDAElB,OAAOH,CAAO,EAIHyJ,EAAmBpJ,IAAmB,IAAlB,SAAEC,GAAUD,EAE3C,MAAOqJ,EAAUC,IAAeC,EAAAA,EAAAA,GAAgB,cAAeN,GAGzDO,GAAgBtH,EAAAA,EAAAA,KAAY,CAACuH,EAAKrC,KAElCqC,KAAOR,GACTK,GAAYhF,IAAI,IACXA,EACH,CAACmF,GAAMrC,KAEX,GACC,CAACkC,IAGEI,GAAgBxH,EAAAA,EAAAA,KAAY,KAChCoH,EAAYL,EAAiB,GAC5B,CAACK,IAGEK,GAA4BzH,EAAAA,EAAAA,KAAayD,KACxCA,KAKgC,IAAnCA,EAAMiE,0BACLjE,EAAMkE,YAAclE,EAAMkE,WAAWC,SAAS,sBAC9CnE,EAAMvB,IAAMuB,EAAMvB,GAAG2F,cAAcC,WAAW,MAC9CrE,EAAMsE,QAAyC,aAA/BtE,EAAMsE,OAAOF,gBAE/B,IAGGzJ,GAA2B4B,EAAAA,EAAAA,KAAayD,GACxCgE,EAA0BhE,GACrB,IACF0D,EACHxD,YAAa,GAGVwD,GACN,CAACA,EAAUM,IAGRvC,GAAQvF,EAAAA,EAAAA,KAAQ,MACpBwH,WACAG,gBACAE,gBACAC,4BACArJ,8BACE,CACF+I,EACAG,EACAE,EACAC,EACArJ,IAGF,OACEyI,EAAAA,EAAAA,GAACI,EAAgBH,SAAQ,CAAC5B,MAAOA,EAAMnH,SACpCA,GACwB,C,qECtM/B,MAAMiK,GAAezK,EAAAA,EAAAA,MAGR0K,EAAWA,KACtB,MAAMxK,GAAUC,EAAAA,EAAAA,IAAWsK,GAC3B,QAAgBrK,IAAZF,EACF,MAAM,IAAIG,MAAM,gDAElB,OAAOH,CAAO,EAIHyK,EAAgBpK,IAAmB,IAAlB,SAAEC,GAAUD,EAExC,MAAOqK,EAAOC,IAAYC,EAAAA,EAAAA,KAAS,IACdC,aAAaC,QAAQ,UACnB,SAIjBC,GAAcxI,EAAAA,EAAAA,KAAY,KAC9BoI,GAASK,IACP,MAAMC,EAAyB,SAAdD,EAAuB,QAAU,OAElD,OADAH,aAAaK,QAAQ,QAASD,GACvBA,CAAQ,GACf,GACD,KAGHE,EAAAA,EAAAA,KAAU,KACRC,SAASxE,KAAKyE,UAAUC,OAAO,aAAc,aAC7CF,SAASxE,KAAKyE,UAAUE,IAAI,GAAGb,SAAa,GAC3C,CAACA,IAGJ,MAAMjD,GAAQvF,EAAAA,EAAAA,KAAQ,MACpBwI,QACAK,cACAS,OAAkB,SAAVd,KACN,CAACA,EAAOK,IAEZ,OACE3B,EAAAA,EAAAA,GAACmB,EAAalB,SAAQ,CAAC5B,MAAOA,EAAMnH,SACjCA,GACqB,C,gFC3C5B,MAAMmL,GAAe3L,EAAAA,EAAAA,MAGR4L,EAAWA,KACtB,MAAM1L,GAAUC,EAAAA,EAAAA,IAAWwL,GAC3B,IAAKzL,EACH,MAAM,IAAIG,MAAM,gDAElB,OAAOH,CAAO,EAIV2L,EAAY,YACZC,EAAe,eAGrB,SAASC,EAAaC,EAAOC,GAC3B,OAAQA,EAAOC,MACb,KAAKL,EACH,MAAO,IAAIG,EAAOC,EAAOhG,SAC3B,KAAK6F,EACH,OAAOE,EAAMG,QAAOC,GAASA,EAAMzH,KAAOsH,EAAOhG,UACnD,QACE,OAAO+F,EAEb,CAGO,MAAMK,EAAgB9L,IAAmB,IAAlB,SAAEC,GAAUD,EACxC,MAAO+L,EAAQC,IAAYC,EAAAA,EAAAA,IAAWT,EAAc,IAG9CU,GAAYhK,EAAAA,EAAAA,KAAYiD,IAAyC,IAAxC,KAAEwG,EAAI,QAAEzI,EAAO,SAAEiJ,EAAW,KAAMhH,EAC/D,MAAMf,EAAKP,KAAKC,MAAMH,WAAaF,KAAKC,SAASC,SAAS,IAAIyI,OAAO,EAAG,GAIxE,OAHAJ,EAAS,CAAEL,KAAML,EAAW5F,QAAS,CAAEtB,KAAIuH,OAAMzI,UAASiJ,cAE1DxH,YAAW,IAAMqH,EAAS,CAAEL,KAAMJ,EAAc7F,QAAStB,KAAO+H,GACzD/H,CAAE,GACR,IAGGiI,GAAenK,EAAAA,EAAAA,KAAYkC,IAC/B4H,EAAS,CAAEL,KAAMJ,EAAc7F,QAAStB,GAAK,GAC5C,IAEH,OACEkI,EAAAA,EAAAA,IAAClB,EAAapC,SAAQ,CAAC5B,MAAO,CAAE8E,YAAWG,gBAAepM,SAAA,CACvDA,GACD8I,EAAAA,EAAAA,GAACwD,EAAAA,EAAc,CAACR,OAAQA,EAAQM,aAAcA,MACxB,C","sources":["contexts/StreamingEventsContext.js","contexts/SettingsContext.js","contexts/ThemeContext.js","contexts/ToastContext.js"],"sourcesContent":["import React, { createContext, useContext, useRef, useCallback, useMemo, useEffect } from 'react';\nimport { useApi } from './ApiContext';\nimport { useModel } from './ModelContext';\nimport { useSettings } from './SettingsContext';\nimport { useAuth } from './AuthContext';\nimport { useChatHistory } from './ChatHistoryContext';\nimport { useChatStatus } from './ChatStatusContext';\nimport { usePerformanceMetrics } from './PerformanceMetricsContext';\nimport { fetchWithRetry } from '../utils/network';\nimport debounce from 'lodash.debounce';\n// Inline worker via worker-loader to avoid separate chunk files\nimport StreamProcessorWorker from '../workers/streamProcessor.js';\n\n// Create a context for streaming events and logic\nconst StreamingEventsContext = createContext();\n\nexport const useStreamingEvents = () => {\n  const context = useContext(StreamingEventsContext);\n  if (context === undefined) {\n    throw new Error('useStreamingEvents must be used within a StreamingEventsProvider');\n  }\n  return context;\n};\n\nexport const StreamingEventsProvider = ({ children }) => {\n  const { apiUrl } = useApi();\n  const { selectedModel } = useModel();\n  const { getModelAdjustedSettings } = useSettings();\n  const { idToken } = useAuth();\n  const { chatHistoryRef, setChatHistory, addMessageToHistory, updateChatWithContent } = useChatHistory();\n  const { setIsWaitingForResponse, setError } = useChatStatus();\n  const { resetPerformanceMetrics, startPerformanceTimer, updatePerformanceMetrics } = usePerformanceMetrics();\n\n  // Refs for streaming\n  const streamingTextRef = useRef('');\n  const currentRequestIdRef = useRef(null);\n  const abortControllerRef = useRef(null);\n  const isStreamingRef = useRef(false);\n  const firstTokenReceivedRef = useRef(false);\n\n  // Debounced content updater\n  const debouncedUpdateChat = useMemo(\n    () => debounce((content) => updateChatWithContent(content), 20),\n    [updateChatWithContent]\n  );\n\n  // SSE parsing worker setup\n  const streamWorkerUrlRef = useRef(null);\n  const streamWorkerRef = useRef(null);\n  const getOrCreateStreamWorker = useCallback(() => {\n    if (!streamWorkerRef.current) {\n      streamWorkerRef.current = new StreamProcessorWorker();\n    }\n    return streamWorkerRef.current;\n  }, []);\n\n  const parseStreamChunk = useCallback((chunk) => new Promise((resolve, reject) => {\n    const worker = getOrCreateStreamWorker();\n    worker.onmessage = (e) => resolve(e.data);\n    worker.onerror = reject;\n    worker.postMessage(chunk);\n  }), [getOrCreateStreamWorker]);\n\n  // Stream a message using fetch SSE\n  const streamMessageWithFetch = useCallback(async (message, editIndex = null) => {\n    // Generate and store a client-side requestId for this stream\n    const requestId = (typeof crypto !== 'undefined' && crypto.randomUUID)\n      ? crypto.randomUUID()\n      : Math.random().toString(36).substring(2) + Date.now().toString(36);\n    currentRequestIdRef.current = requestId;\n    const isEditing = editIndex !== null && Number.isInteger(editIndex) && editIndex >= 0;\n    if (!message || !selectedModel) {\n      setError('Please enter a message and select a model');\n      return null;\n    }\n    const modelId = `${selectedModel.provider}/${selectedModel.id}`;\n    if (!modelId) {\n      setError('Invalid model selection');\n      return null;\n    }\n    let userMessage;\n    if (isEditing) {\n      setChatHistory(prev => {\n        const truncated = prev.slice(0, editIndex);\n        const original = prev[editIndex];\n        // Preserve original id/timestamp, only update content\n        userMessage = { ...original, content: message };\n        return [...truncated, userMessage];\n      });\n    } else {\n      userMessage = addMessageToHistory('user', message);\n    }\n    resetPerformanceMetrics();\n    startPerformanceTimer();\n    firstTokenReceivedRef.current = false;\n    setIsWaitingForResponse(true);\n    setError(null);\n    streamingTextRef.current = '';\n    isStreamingRef.current = true;\n    addMessageToHistory('assistant', '');\n    let timeoutId = setTimeout(() => {\n      abortControllerRef.current?.abort('timeout');\n      setError('Connection timed out');\n      setIsWaitingForResponse(false);\n    }, 60000);\n    const abortController = new AbortController();\n    abortControllerRef.current = abortController;\n    try {\n      const adjusted = getModelAdjustedSettings(selectedModel);\n      // Build API history without the placeholder assistant message (empty content)\n      const historyForApi = chatHistoryRef.current\n        .slice(0, -1)\n        .map(({ metrics, ...m }) => m);\n      if (adjusted.systemPrompt && (!historyForApi.length || historyForApi[0].role !== 'system')) {\n        historyForApi.unshift({ role: 'system', content: adjusted.systemPrompt, timestamp: Date.now() - 1 });\n      }\n      const payload = {\n        requestId,\n        model: modelId,\n        messages: historyForApi,\n        temperature: adjusted.temperature,\n        max_tokens: adjusted.max_tokens,\n        top_p: adjusted.top_p,\n        frequency_penalty: adjusted.frequency_penalty,\n        presence_penalty: adjusted.presence_penalty\n      };\n      const headers = { 'Content-Type': 'application/json', 'Accept': 'text/event-stream', 'Cache-Control': 'no-cache' };\n      if (idToken) headers['Authorization'] = `Bearer ${idToken}`;\n      const response = await fetchWithRetry(new URL('/api/chat/stream', apiUrl).toString(), {\n        method: 'POST', headers, body: JSON.stringify(payload), signal: abortController.signal, cache: 'no-store'\n      });\n      if (!response.ok) throw new Error(`API error: ${response.status}`);\n      const reader = response.body.getReader();\n      const decoder = new TextDecoder('utf-8');\n      let accumulatedContent = '';\n      while (true) {\n        const { done, value } = await reader.read();\n        clearTimeout(timeoutId);\n        timeoutId = setTimeout(() => {\n          abortControllerRef.current?.abort();\n          setError('Connection timed out');\n          setIsWaitingForResponse(false);\n        }, 60000);\n        if (done) {\n          // handle leftover buffer\n          break;\n        }\n        const chunk = decoder.decode(value, { stream: true });\n\n        console.log('Received stream chunk:', chunk);\n\n        try {\n          const msgs = await parseStreamChunk(chunk);\n          for (const msg of msgs) {\n            // Handle server-sent error payload\n            if (msg.rawChunk?.error || msg.finishReason === 'error') {\n              const errMsg = msg.rawChunk?.error?.message || 'Error occurred during generation';\n              console.error('Error in SSE payload:', errMsg);\n              setError(errMsg);\n              setChatHistory(prev => {\n                const newHistory = [...prev];\n                const lastMsg = newHistory[newHistory.length - 1];\n                if (lastMsg && lastMsg.role === 'assistant') {\n                  lastMsg.content += `\\n\\n**Error:** ${errMsg}`;\n                  if (lastMsg.metrics) {\n                    lastMsg.metrics.isComplete = true;\n                    lastMsg.metrics.error = true;\n                  }\n                }\n                return newHistory;\n              });\n              return null;\n            }\n            // Append any content from the chunk\n            if (msg.content) {\n              // Record time to first token once\n              if (!firstTokenReceivedRef.current) {\n                updatePerformanceMetrics(1);\n                firstTokenReceivedRef.current = true;\n              }\n              accumulatedContent += msg.content;\n              streamingTextRef.current = accumulatedContent;\n              debouncedUpdateChat(accumulatedContent);\n            }\n            // Always use server-reported completion tokens for metrics\n            const completionTokens = msg.usage?.completionTokens ?? 0;\n            updatePerformanceMetrics(completionTokens, msg.isDone, msg.usage, msg.finishReason);\n          }\n        } catch {}\n      }\n      debouncedUpdateChat.flush();\n      updateChatWithContent(streamingTextRef.current);\n      return streamingTextRef.current;\n    } catch (error) {\n      console.error('Error streaming message:', error);\n      setError(error.message);\n      // Show the server error content as the assistant's message\n      setChatHistory(prev => {\n        const newHistory = [...prev];\n        const lastMsg = newHistory[newHistory.length - 1];\n        if (lastMsg && lastMsg.role === 'assistant') {\n          lastMsg.content += `\\n\\n**Error:** ${error.message || 'Error occurred during generation'}`;\n          if (lastMsg.metrics) {\n            lastMsg.metrics.isComplete = true;\n            lastMsg.metrics.error = true;\n          }\n        }\n        return newHistory;\n      });\n      return null;\n    } finally {\n      clearTimeout(timeoutId);\n      isStreamingRef.current = false;\n      setIsWaitingForResponse(false);\n      // Do not auto-call stop endpoint here; only explicit stop should trigger it\n      currentRequestIdRef.current = null;\n    }\n  }, [\n    apiUrl, selectedModel, getModelAdjustedSettings, idToken,\n    chatHistoryRef, setChatHistory, addMessageToHistory, updateChatWithContent,\n    debouncedUpdateChat, setError, setIsWaitingForResponse,\n    resetPerformanceMetrics, startPerformanceTimer, updatePerformanceMetrics,\n    parseStreamChunk\n  ]);\n\n  const stopStreaming = useCallback(async () => {\n    if (abortControllerRef.current) abortControllerRef.current.abort('user_stopped');\n    const reqId = currentRequestIdRef.current;\n    if (reqId) {\n      const headers = { 'Content-Type': 'application/json' };\n      if (idToken) headers['Authorization'] = `Bearer ${idToken}`;\n      try {\n        await fetchWithRetry(new URL('/api/chat/stop', apiUrl).toString(), {\n          method: 'POST', headers, body: JSON.stringify({ requestId: reqId })\n        });\n      } catch {} finally {\n        currentRequestIdRef.current = null;\n        abortControllerRef.current = null;\n      }\n    }\n    isStreamingRef.current = false;\n    setIsWaitingForResponse(false);\n    return true;\n  }, [apiUrl, idToken, setIsWaitingForResponse]);\n\n  const value = useMemo(() => ({\n    streamMessageWithFetch,\n    stopStreaming,\n    parseStreamChunk,\n    streamingTextRef,\n    isStreaming: () => isStreamingRef.current\n  }), [streamMessageWithFetch, stopStreaming, parseStreamChunk]);\n\n  return (\n    <StreamingEventsContext.Provider value={value}>\n      {children}\n    </StreamingEventsContext.Provider>\n  );\n}; ","import { createContext, useContext, useCallback, useMemo } from 'react';\nimport { useLocalStorage } from '../hooks/useLocalStorage';\n\n// Default settings values\nconst DEFAULT_SETTINGS = {\n  temperature: 0.7,\n  top_p: 1.0,\n  max_tokens: 8191,\n  frequency_penalty: 0,\n  presence_penalty: 0,\n  streaming: true,\n  systemPrompt: `# 🧠 System Instruction: University-Level AI Assistant Guide\n\n**You are a knowledgeable, friendly, and supportive university-level assistant.**\nYour mission is to help students understand complex topics with clarity, encouragement, and structure—like a brilliant but approachable mentor or senior student.\n\n---\n\n## ✨ Core Principles for Every Response\n\nTo ensure clarity, efficiency, and student comprehension, adhere to these fundamental principles in **every response**:\n\n*   **📊 Prioritize Visuals over Dense Text:** **Always try to use tabular or other visual information in place of text if possible.** Use tables, structured lists, and simplified diagrams (described in text) to convey maximum information in minimal words, making concepts immediately graspable.\n*   **🎯 Direct, Concise & Focused:** **Only answer what is directly asked for, and do not explain in detail unless explicitly requested** (e.g., \"explain in detail,\" \"tell me more\"). Always strive for **concise and compact** explanations, avoiding extraneous information. Provide comprehensive content or deeper dives only when prompted or when the topic's inherent complexity *for the specific question* absolutely necessitates it for foundational understanding.\n*   **📐 Mandatory Math & Code Formatting:** Correct markdown formatting for **mathematical formulas ($ $$)** and **code blocks** is strictly required whenever they are included.\n\n---\n\n## 📝 Response Structure & Content Guidelines\n\nWhile highly adaptable to the specific query, aim to incorporate these elements for optimal learning:\n\n### ✅ 1. Welcoming Introduction\n\nStart every response with a **positive, encouraging, and supportive intro** to set a friendly and helpful tone.\n\n> *Examples:*\n> *   \"Alright! Let's break this down together—I'll explain everything step by step with clear examples and helpful tips!\"\n> *   \"You've got this! Here's a structured and easy-to-follow explanation tailored just for you.\"\n\n### 2. Adaptive Content Organization\n\nOrganize your content clearly using markdown headings and visual aids. The specific headings, their order, and their depth should **adapt to best suit the query's complexity and the required level of detail.**\n\n*   **Heading Hierarchy:** Use markdown heading levels ('##', '###', '####', etc.) to create a clear, logical, and multi-level hierarchy. **Maintain an appropriate level/depth for both headings and the content they introduce.** Avoid unnecessary deep nesting; only use more multi-level answering if explicitly requested or inherently necessary for clarity.\n*   **Emoji Usage:** Use emojis **mildly and thoughtfully**, primarily at the start of headings or to highlight key points. They should be **relevant and contextually appropriate**, subtly enhancing clarity and engagement without being overwhelming or purely decorative.\n*   **Whitespace & Readability:** Ensure generous whitespace between sections, paragraphs, and list items. Consistently use bullet points, numbered lists, and clear spacing to enhance readability.\n\n### 3. Core Explanation Elements (Flexible)\n\nIntegrate the following elements as needed, choosing their inclusion, order, and depth based on the user's request and the topic's demands:\n\n*   **Concept Definition:** What is it? Why does it matter? Define jargon.\n*   **Operational Details:** How does it work? Practical applications.\n*   **Examples:** Concrete instances, including formulas or code.\n*   **Key Insights:** Tips, best practices, common pitfalls, comparisons.\n\n### 4. Math & Code Formatting\n\n*   **📐 Math:**\n    *   Inline: '$E = mc^2$'\n    *   Block:\n        '\n        $$\n        E = mc^2\n        $$\n        '\n\n### 5. Clear Concluding Sections\n\nConclude each response with **one** summary section, using bold headers and bullets:\n\n*   '# ✅ Key Takeaways' — *Concise summary for revision.*\n*   '# 🔍 Next Steps / Related Topics' — *Suggestions for deeper exploration or application.*\n\n### 6. Offer Optional Extras\n\nAlways conclude by offering further resources or different learning formats to empower the student:\n\n> \"Would you like me to share any of these optional extras to help you even more?\"\n> *   📋 A summary table\n> *   💻 Specific code snippets\n> *   🧠 Quick revision notes\n> *   📘 Further reading suggestions\n\n---\n\n## 🗣️ Tone & Formatting Principles\n\n*   **🧑‍🏫 Tone:** Friendly, motivating, and peer-like—never robotic or dry. Aim for an approachable, expert voice.\n*   **💡 Accuracy:** Ensure all information is academically sound and precise.\n*   **🧼 Formatting Details:**\n    *   **Bold** for emphasis.\n    *   *Italics* for subtle notes or definitions.\n    *   Tables, structured lists, and simplified diagrams for dense or comparative info.\n    *   Emojis used thoughtfully to enhance context.\n\n---\n## 🌟 The Golden Rule\n\n**Prioritize clarity, accuracy, and student comprehension above all else.** Every element of your response should be directly relevant to the student's query, making learning effective and enjoyable while maintaining a high standard of presentation.`\n//   `You are a knowledgeable, friendly, and supportive university-level assistant.\n\n// For every question or topic, provide a clear, engaging, and well-structured answer, styled like an expert mentor or senior student.\n\n// Style and Structure:\n\n// Begin with a welcoming, positive intro (e.g., \"Alright! I'll break this down for you in detail section by section, with clear explanations and important points.\").\n// Organize your response into numbered sections, each with a descriptive header and an emoji (e.g., # 📚 1. Core Concept).\n// In each section, explain:\n// Core ideas and definitions\n// How things work (step-by-step, or process overview)\n// Any relevant formulas, code, or examples\n// Key points, tips, or comparisons\n// Use subheadings, bullet points, tables, and diagrams (ASCII or LaTeX) for clarity when helpful.\n// At the end, summarize with a \"Key Takeaways\" or \"Next Steps/Related Topics\" section, with quick revision notes, further reading, or suggestions for deeper exploration if relevant.\n// Always offer to provide summary tables, code snippets, or quick revision notes if the user wants them.\n// Tone: Friendly, supportive, and approachable—like a helpful peer or mentor. Formatting: Use bold, italics, emojis, markdown headers, and tables to maximize clarity.\n\n// Use emojis befitting the context\n\n// Your goal: Make complex ideas easy to understand, memorable, and actionable for the student—whether for study, projects, or curiosity.`\n//  systemPrompt: \"You are ChatGPT, a helpful and knowledgeable AI assistant. Your primary role is to assist Nikhil, a university engineering student, by providing clear, concise, and technically accurate information. Adopt a friendly and approachable tone, akin to a knowledgeable peer or mentor. Enhance your responses with relevant emojis to convey tone and emotion, making interactions more engaging. Structure your answers logically, using bullet points or numbered lists where appropriate to enhance clarity. When applicable, incorporate interactive elements such as code snippets or diagrams to facilitate deeper understanding. Encourage curiosity by suggesting related topics or questions that Nikhil might explore further. Always tailor your assistance to support Nikhil's academic and personal growth in the field of engineering\"\n};\n\n// Create settings context\nconst SettingsContext = createContext();\n\n// Custom hook for using settings\nexport const useSettings = () => {\n  const context = useContext(SettingsContext);\n  if (context === undefined) {\n    throw new Error('useSettings must be used within a SettingsProvider');\n  }\n  return context;\n};\n\n// Settings provider component\nexport const SettingsProvider = ({ children }) => {\n  // Initialize settings state with defaults, persisted to localStorage\n  const [settings, setSettings] = useLocalStorage('appSettings', DEFAULT_SETTINGS);\n  \n  // Handle individual setting updates\n  const updateSetting = useCallback((key, value) => {\n    // Ensure the key is a valid setting we manage\n    if (key in DEFAULT_SETTINGS) {\n      setSettings(prev => ({\n        ...prev,\n        [key]: value\n      }));\n    }\n  }, [setSettings]);\n  \n  // Reset settings to defaults\n  const resetSettings = useCallback(() => {\n    setSettings(DEFAULT_SETTINGS);\n  }, [setSettings]);\n  \n  // Check if temperature should be restricted based on model name/series\n  const shouldRestrictTemperature = useCallback((model) => {\n    if (!model) return false;\n    \n    // More explicit flag checking for temperature restriction\n    // Check for specific model properties that indicate temperature restriction\n    return (\n      model.requiresFixedTemperature === true || \n      (model.properties && model.properties.includes('fixed_temperature')) ||\n      (model.id && model.id.toLowerCase().startsWith('o')) ||\n      (model.series && model.series.toLowerCase() === 'o-series')\n    );\n  }, []);\n  \n  // Get current settings with potential model-specific overrides\n  const getModelAdjustedSettings = useCallback((model) => {\n    if (shouldRestrictTemperature(model)) {\n      return {\n        ...settings,\n        temperature: 1.0\n      };\n    }\n    return settings;\n  }, [settings, shouldRestrictTemperature]);\n  \n  // Memoize context value to prevent unnecessary re-renders\n  const value = useMemo(() => ({\n    settings,\n    updateSetting,\n    resetSettings,\n    shouldRestrictTemperature,\n    getModelAdjustedSettings\n  }), [\n    settings,\n    updateSetting, \n    resetSettings, \n    shouldRestrictTemperature, \n    getModelAdjustedSettings\n  ]);\n  \n  return (\n    <SettingsContext.Provider value={value}>\n      {children}\n    </SettingsContext.Provider>\n  );\n}; ","import { createContext, useContext, useState, useEffect, useCallback, useMemo } from 'react';\n\n// Create theme context\nconst ThemeContext = createContext();\n\n// Custom hook for using theme\nexport const useTheme = () => {\n  const context = useContext(ThemeContext);\n  if (context === undefined) {\n    throw new Error('useTheme must be used within a ThemeProvider');\n  }\n  return context;\n};\n\n// Theme provider component\nexport const ThemeProvider = ({ children }) => {\n  // Initialize theme from localStorage or default to 'dark'\n  const [theme, setTheme] = useState(() => {\n    const savedTheme = localStorage.getItem('theme');\n    return savedTheme || 'dark';\n  });\n\n  // Toggle between light and dark themes\n  const toggleTheme = useCallback(() => {\n    setTheme(prevTheme => {\n      const newTheme = prevTheme === 'dark' ? 'light' : 'dark';\n      localStorage.setItem('theme', newTheme);\n      return newTheme;\n    });\n  }, []);\n\n  // Apply theme class to body element\n  useEffect(() => {\n    document.body.classList.remove('light-mode', 'dark-mode');\n    document.body.classList.add(`${theme}-mode`);\n  }, [theme]);\n\n  // Context value - memoized to prevent unnecessary re-renders\n  const value = useMemo(() => ({\n    theme,\n    toggleTheme,\n    isDark: theme === 'dark'\n  }), [theme, toggleTheme]);\n\n  return (\n    <ThemeContext.Provider value={value}>\n      {children}\n    </ThemeContext.Provider>\n  );\n}; ","import React, { createContext, useContext, useReducer, useCallback } from 'react';\nimport ToastContainer from '../components/common/ToastNotification';\n\n// Create context for toast notifications\nconst ToastContext = createContext();\n\n// Hook to use toast context\nexport const useToast = () => {\n  const context = useContext(ToastContext);\n  if (!context) {\n    throw new Error('useToast must be used within a ToastProvider');\n  }\n  return context;\n};\n\n// Action types\nconst ADD_TOAST = 'ADD_TOAST';\nconst REMOVE_TOAST = 'REMOVE_TOAST';\n\n// Reducer to manage toast list\nfunction toastReducer(state, action) {\n  switch (action.type) {\n    case ADD_TOAST:\n      return [...state, action.payload];\n    case REMOVE_TOAST:\n      return state.filter(toast => toast.id !== action.payload);\n    default:\n      return state;\n  }\n}\n\n// Provider component that holds toast state and renders toasts\nexport const ToastProvider = ({ children }) => {\n  const [toasts, dispatch] = useReducer(toastReducer, []);\n\n  // Function to show a toast\n  const showToast = useCallback(({ type, message, duration = 3000 }) => {\n    const id = Date.now().toString() + Math.random().toString(36).substr(2, 9);\n    dispatch({ type: ADD_TOAST, payload: { id, type, message, duration } });\n    // Auto-dismiss toast after duration\n    setTimeout(() => dispatch({ type: REMOVE_TOAST, payload: id }), duration);\n    return id;\n  }, []);\n\n  // Function to manually dismiss a toast\n  const dismissToast = useCallback(id => {\n    dispatch({ type: REMOVE_TOAST, payload: id });\n  }, []);\n\n  return (\n    <ToastContext.Provider value={{ showToast, dismissToast }}>\n      {children}\n      <ToastContainer toasts={toasts} dismissToast={dismissToast} />\n    </ToastContext.Provider>\n  );\n}; "],"names":["StreamingEventsContext","createContext","useStreamingEvents","context","useContext","undefined","Error","StreamingEventsProvider","_ref","children","apiUrl","useApi","selectedModel","useModel","getModelAdjustedSettings","useSettings","idToken","useAuth","chatHistoryRef","setChatHistory","addMessageToHistory","updateChatWithContent","useChatHistory","setIsWaitingForResponse","setError","useChatStatus","resetPerformanceMetrics","startPerformanceTimer","updatePerformanceMetrics","usePerformanceMetrics","streamingTextRef","useRef","currentRequestIdRef","abortControllerRef","isStreamingRef","firstTokenReceivedRef","debouncedUpdateChat","useMemo","debounce","content","streamWorkerRef","getOrCreateStreamWorker","useCallback","current","StreamProcessorWorker","parseStreamChunk","chunk","Promise","resolve","reject","worker","onmessage","e","data","onerror","postMessage","streamMessageWithFetch","async","message","editIndex","arguments","length","requestId","crypto","randomUUID","Math","random","toString","substring","Date","now","isEditing","Number","isInteger","modelId","provider","id","userMessage","prev","truncated","slice","original","timeoutId","setTimeout","_abortControllerRef$c","abort","abortController","AbortController","adjusted","historyForApi","map","_ref2","metrics","m","systemPrompt","role","unshift","timestamp","payload","model","messages","temperature","max_tokens","top_p","frequency_penalty","presence_penalty","headers","response","fetchWithRetry","URL","method","body","JSON","stringify","signal","cache","ok","status","reader","getReader","decoder","TextDecoder","accumulatedContent","done","value","read","clearTimeout","_abortControllerRef$c2","decode","stream","msgs","msg","_msg$rawChunk","_msg$usage$completion","_msg$usage","rawChunk","error","finishReason","_msg$rawChunk2","_msg$rawChunk2$error","errMsg","newHistory","lastMsg","isComplete","completionTokens","usage","isDone","flush","stopStreaming","reqId","isStreaming","_jsx","Provider","DEFAULT_SETTINGS","streaming","SettingsContext","SettingsProvider","settings","setSettings","useLocalStorage","updateSetting","key","resetSettings","shouldRestrictTemperature","requiresFixedTemperature","properties","includes","toLowerCase","startsWith","series","ThemeContext","useTheme","ThemeProvider","theme","setTheme","useState","localStorage","getItem","toggleTheme","prevTheme","newTheme","setItem","useEffect","document","classList","remove","add","isDark","ToastContext","useToast","ADD_TOAST","REMOVE_TOAST","toastReducer","state","action","type","filter","toast","ToastProvider","toasts","dispatch","useReducer","showToast","duration","substr","dismissToast","_jsxs","ToastContainer"],"sourceRoot":""}